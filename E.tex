\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{appendix}
%\newcommand{\pd}[2][]{\frac{\partial#1}{\partial#2}}

\title{\textbf{Temperature of Output Layer rather than Distillation Technique Determines the Robustness of Convolutional Neural Networks against Adversarial Perturbations}}
\author{Haobei Song \\
        University of Waterloo}
\date{March 26 2017}
\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

\begin{abstract}
	Convolutional neural network (CNN) as a well developed deep learning architecture has been widely used in computer vision such as automatic inspection, autonomous driving, image processing.
	Though state-of-art accuracy by elaborately designed CNN was achieved in many computer vision tasks, 
	recent studies have shown the potential vulnerability to adversarial perturbations among not only CNN but other deep neural networks.
	This discovery is of substantial significance as the widely usage of Convolutional neural network targets tasks of extensive security concern
	such as the CNN used in autonomous driving where the car might be crushed by slight modification of the environment. 
	In this study, the defensive effect of distillation training for CNN is evaluated together with traditional trained CNN with and without temperature setting for MNIST task on a data set of 60,000 examples.
	The result shows it is the temperature (T) of the softmax function of the output layer that turns out to be the main factor in reducing the adversarial gradient (by a factor of $10^{10}$ at $T=20$) and the success rate of adversarial attack (by a factor of 10 when $T=20$),
 rather than the distillation training technique claimed effective by other researchers.
\end{abstract}
\section{Introduction}
Considering when humans learn to recognize the digits written on a piece of paper, the input is the "viewing" of that image or the pixels on the paper with someone by their side teaching them the right number that image represents.
People can also teach a computer how to do this by buiding a deep neural network and let it learn the parameters characterizing such a task from a mass of data (hand written digits with labels) without explicitly crafting the algorithm.
Normally, a deep neural network is built with some initial parameters to be modified to fit the given data set and to generalize well when applied to new data. Learning the hand written digits is a classical deep learning task widely referred to as MNIST, which is often done by convolutional neural networks (CNN).

Ever since the introduction of convolutional neural networks for image processing tasks, the recognition accuracy has increased dramatically and achieved state-of-art accuracy in the past few years. Though computers now could outperform humans on this specific task,
recent study has shown the lack of robustness of CNN against adversarial perturbations which raises plenty of problems about its pragmatic application. 

Compared with training computers to solve MNIST problem, humans during training process could learn more information than the hard labels given, such as the similarities between different digits, 
which is some information left out by traditional training with hard label. 
Theoretically, a deep enough neural network could learn such extra information when trained on a large enough scale of data. Such an ideal neural network does not exist so far due to the limited data people can obtain and computation ability required to perform the training on such neural network, as there is considerable complexity even within the simplest learning task such as MNIST\cite{schmid}. Humans often make use of knowledge from a variety of areas such as math, culture or even their personal experience to deal with MNIST tasks. For example, people can easily recognize rotated hand-written digits right after the training of upright digits from geometry or arabs could find some correlation from their language etc.
Thus, specifying some hyper-parameters of the training model is considerably necessary for building a DNN to solve such a realistic problem.

Though soft label method during training process has been suggested but crafting such soft label is also questionable as there is no general rule to create labels which perameterize the relationship of hand-written digits falling into different categories as perceived by humans.
It also requires a considerable amount of effort to do these tedious task without a systematic rule, which disobeys the principle of machine learning that simply tries to avoid these tedious work.
This is where distillation comes to the stage as to provide distilled labels from previously built CNN. The distilled labels can be considered as a kind of soft labels produced by computer, which labels each sample with a soft label using a traditional or regular CNN without distillation or temperature setting. 
Papernot, McDaniel and Wu etc. have claimed its effectiveness against adversarial perturbation from empirical study in \cite{Papernot}.
In their work, they also applied a softmax output layer parameterized by a parameter called temperature, which turned out to be the most important factor reducing the success rate of adversarial attack in our study and thus the effectiveness of distillation solely becomes questionable.

From a set of distilled labels, a deep learning neural network can gain more information such as the relationship between different handwritten digits explicitly. However, the information extracted by the previously built CNN is indeed part of information of the database training that CNN.
As a result, the distillation training method places a considerably rigorous requirement on the CNN that produces those distilled labels.
Theoretically, CNN built upon distilled labels can never outperform the previously built CNN either in prediction accuracy or robustness against adversarial perturbations based on the assumption that these CNNs make best use of every sample in the database. 
Though, it is of appreciable pragmatic significance as in reality, the database is always limited and the computational ability is just a trace of that of human's brain.
That is why distillation is initially used to train a deep learning neural network on a comparablly computational resource limited devices such as phones, which can then use the preprocessed or distilled labels produced from a DNN on a much powerful computer.

In this paper, we investigated the effectiveness of distillation applied to the classical MNIST classification problem as a defense to adversarial perturbations. We adopted three different algorithms to craft adversarial samples and compared the success rate to make the CNN produce a different prediction from the target.
However, in our study, the traditional CNN with temperature control turned out to perform as well as or outperform the CNN using distillation in both the robustness against adversarial perturbations and the prediction accuracy. 

\section{Convolutional Neural Networks for MNIST}
MNIST is a database of 70,000 images of handwritten digits with the labels specifiing the actual numbers on each image percieved by humans\cite{MNIST}. MNIST as a subset of NIST contains normalized arrays of integers representing handwritten digits of 28 $\times$ 28 black and white pixels, along with a target integer indicating the number out of $0\sim 9$ each image represents.
The 70,000 examples are divided into a test set of 10,000 samples and a training set, which is further divided into a validation set of 10,000 samples and the rest as the actual training set of size 50,000.

The architecture of the CNN used in this work resembles the CNN built by \cite{Papernot} in order to verify the effectiveness they claimed of distillation as a defense to adversarial attacks against deep neural networks.\cite{MNIST} 

\begin{center}
	\begin{table}[h!]
	{\large \textbf{Architecture of the CNN for MNIST}
	\\}

	\begin{tabular*}{\textwidth}{{c}{c}{c}}
	\\ \hline \hline
		Layer Type & Nonlinearity &Characteristics\\ \hline
		Input layer & N/A & N/A \\
		Convolutional layer & Rectified linear unit & 32 filters ($3\times3$)\\
		Convolutional layer & Rectifi3d linear unit & 32 filters ($3\times3$)\\
		Max Pooling layer & N/A & $2\times2$\\
		Convolutional layer & Rectified linear unit & 64 filters ($3\times3$)\\
		Convolutional layer & Rectified linear unit & 64 filters ($3\times3$)\\
		Max Pooling layer & N/A & $2\times2$\\
		Dropout layer & N/A & Dropout rate 0.5\\
		Fully Connected layer & Rectified linear unit & 200 units\\
		Dropout layer & N/A & Dropout rate 0.5\\
		Fully Connected layer & Rectified linear unit & 200 units\\
		Dropout layer & N/A & Dropout rate 0.5\\
		Fully Connected layer & softmax & 10 units\\
		\hline
		\\
	\end{tabular*}
	{\large
	\textbf{Training parameters}}

	\begin{tabular*}{\textwidth}{cc}
		\\ \hline \hline
		Parameter & value \\
		\hline
		Learning Rate & 0.1 (0.05 for distillation training)\\
		Momentum & 0.5\\
		Batch & 128 samples\\
		Epochs & 50 \\
		Temperature (except for the standard CNN) & 20 \\ 
		GPU model & NVIDIA GTX 1060 \\ hline
		GPU generation toolkit & cuda-8.0 \\ \hline

	\end{tabular*}
	\caption{CNN architecture overview}
\end{table}
\end{center}

The convolutional network above is implemented by Lasagne\cite{Lasagne} which is built upon Theano\cite{Theano} to generate computational graph in order to accelerate computation during training and the later testing.
We adopted latest GPU generation code on NVIDIA GTX 1060 graphic card with 6 GB GDDR5 to reduce the time required for each training so as to be able to compare the robustness against adversarial perturbations when hyperparameters change.
All the code used in this paper is available on github \url{https://github.com/songhobby/CNN.git}

\section{Effect of Adversarial Perturbations against CNN}
Machine learning security is a recently emerging problem after the state-of-art accuracy has been achieved in many learning tasks. It was proposed when people noticed the vulnerability of DNN exposed to adversarial attacks in the form of small perturbations, which however can be easily differentiated by humans.
\subsection{Adversarial Perturbations}
\paragraph{Definition 1.}
An \textbf{adversarial sample} is an input $x\prime$ obtained by some degree of perturbation from an input $x$ to a prediction model $M$ with $M(x)=y$, where $y$ is the target output, such that $M(x\prime) \ne y$.

The following are examples of adversarial samples obtained from examples in the MNIST database by slight perturbations determined (see \textbf{Algorithm 1.}) from the gradient with respect to the cross-entropy (loss) of each sample on every pixel. 
\begin{figure}[h!]
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{sample_original1.png}
		\caption{Original examples}
	\end{minipage} \hfill
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{sample_perturbed1.png}
		\caption{Adversarial samples}
	\end{minipage} 
\end{figure}
\newpage
\begin{figure}[h!]
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{sample_original2.png}
		\caption{Original examples}
	\end{minipage} \hfill
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{sample_perturbed2.png}
		\caption{Adversarial samples}
	\end{minipage}
\end{figure}

For traditional CNN, the success rate is $66\%$ with maximum 100 pixels perturbed using adversarial sample crafting \textbf{algorithm 1}, and the success rate of adversarial attacking is more than $95\%$ by \textbf{algorithm 2} and \textbf{algorithm 3} introduced in the following section.

\subsection{Adversarial Sample Crafting Algorithms}
\paragraph{Algorithm 1.}
\begin{enumerate}

\item Randomly select a sample input with its target output and make a copy of that input array denoted as $c$

\item Build a saliency map $S$ mapping each index $S[m][n]$ to the gradient of the cross-entropy (loss function) with respect to the corresponding input $c[m][n]$, which as a whole forms a 2D array. (see an example in \textbf{Appendix A}

\item Iterate for trial from $i=1$:

\begin{enumerate}
	\item Find the maximum number $k_i[m][n]$ in the saliency map.

	\item Set $k_i[m][n]$ to minus infinity  and $c[m][n]$ to the maximum.

	\item Feed $c$ to the trained CNN and check if the output differs from the target output:
		\begin{enumerate}
			\item Yes. Terminate the loop and output i
			\item No. Set $i=i+1$. If $i > 100$, terminate and output "Failed". Otherwise, go back to step \textbf{(a).}
		\end{enumerate}
\end{enumerate}
\end{enumerate}

This algorithm was proposed by Papernot\cite{Papernot} in 2016 to simplify the process of crafting adversarial samples. It focuses solely on the pixels of positive large gradient with respect to the cross-entropy, but the information contained in the pixels with negative as large gradient is left out.
In addition, this algorithm would fail when the gradient of original pixels picked by saliency map is close to the maximum.\cite{Papernot2}

Let $Grad(x_[m][n])$ be the gradient of loss function on $[m][n]$ of the input array, we have 
\begin{equation}
	\Delta loss \approx Grad(x[m][n])\Delta x[m][n]
\end{equation}
$\Delta loss$ would not change too much with small $\Delta x[m][n]$ even when $Grad(x[m][n])$ is large.

Therefore, we implement the following algorithm to make use of most of the information from the input array.

\paragraph{Algorithm 2.}
\begin{enumerate}
	\item Randomly select a sample input and make a copy of it as $c$, plug $c$ into the loss function $L$ and calculate the loss of the example. Denote it as $loss_0=L(c)$ and let $S_1,S_2$ represent two saliency maps.
	\item Iterate through the input array and for any $m,n$
		\begin{enumerate}
			\item $c[m][n]=temp$
			\item $c[m][n]=1(maximum)$
			\item $S_1[m][n]= \mid L(c) - loss_0 \mid$
			\item $c[m][n]=0(minimum)$
			\item $S_2[m][n]=\mid L(c) - loss_0 \mid$
			\item $c[m][n]=temp$
		\end{enumerate}

	\item Iterate for trial from $i=1$:
		\begin{enumerate}
			\item Find $M_i=max\{S_1[m][n],S_2[m][n]\}$ in the saliency map.
				\begin{enumerate}
					\item if $M_i==S_1[m][n]$, $c[m][n]=1(maximum)$
					\item if $M_i==S_2[m][n]$, $c[m][n]=0(minimum)$
				\end{enumerate}
			\item Feed $c$ to the trained CNN and check if the output differs from the target output:
				\begin{enumerate}
					\item Yes. Terminate the loop and output i
					\item No. Set $i=i+1$. If $i > 100$, terminate and output "Failed". Otherwise, go back to step \textbf{(a).}
				\end{enumerate}
		\end{enumerate}
\end{enumerate}

This algorithm captures most of the information in the input array as it makes use both positive and negative large gradients. It also takes into account the amplitude of the changing step for each input pixel to maximize the equation (1).

However, the computational cost or time efficiency cost is $\mathcal{O}(N+M)$ where $N$ is the length of the input array and $M$ is the limit of the number of pixels to be modified. 
But the \textbf{algorithm 1} is of $\mathcal{O}(N)$ if we assume the cost for computing gradient on a pixel the same as the cost for evaluating the loss function once.

We can actually sacrifice a little accuracy to obtain a $\mathcal{O}(N)$ efficiency if we ignore the effect $\Delta x[m][n]$ has on $\Delta loss$ and only take into account the negative large gradient.

\paragraph{Algorithm 3.}
\begin{enumerate}
\item Randomly select a sample input with its target output and make a copy of that input array denoted as $c$

\item Build a saliency map $S$ mapping each index $S[m][n]$ to the gradient of the cross-entropy (loss function) with respect to the corresponding input $c[m][n]$, which as a whole forms a 2D array. (see an example in \textbf{Appendix A}

\item Iterate for trial from $i=1$:

\begin{enumerate}
	\item Find the number $k_i[m][n]$ with maximum absolute value in the saliency map.

	\item Set $k_i[m][n]$ to 0 , and depending on the sign of $k_i[m][n]$, set $c[m][n]$ to 1 (the maximum) if $k_i[m][n] > 0$ and 0 (the minimum) otherwise ($k_i[m][n] < 0$).

	\item Feed $c$ to the trained CNN and check if the output differs from the target output:
		\begin{enumerate}
			\item Yes. Terminate the loop and output i
			\item No. Set $i=i+1$. If $i > 100$, terminate and output "Failed". Otherwise, go back to step \textbf{(a).}
		\end{enumerate}
\end{enumerate}
\end{enumerate}

These three algorithms are implemented in our work and the code is availabale at \url{https://github.com/songhobby/CNN.git} named as test1,py, test3,py and test2.py.

Though, this is not the end of story. A more effective algorithm is to calculate the saliency map every time an index of input array is modified. But the computational cost is of $\mathcal{O}(N^2)$.
\paragraph{Algorithm 4.}
Based on \textbf{Algorithm 2.}, update the saliency maps each time an input array index is modified.
\section{Temperature vs. Distillation against Adversarial Perturbations}

\paragraph{Definition 2.}
The \textbf{temperature} of a softmax layer (usually used as an output layer of a DNN) is defined as the parameter $T$, which is greater than $1$, such that the softmax function with temperature becomes 
\begin{equation}
	f(\mathbf{x_i};T)=\frac{e^{\frac{\mathbf{x_i}}{T}}}{\sum_{j=1}^n 
	e^{\frac{\mathbf{x_j}}{T}}}
\end{equation}

We can show that as temperature increases, the sensitivity to perturbations of pixels decreases tremendously.

\paragraph{Proof 1.}
Let $\mathbf{x}$ be the input vector to the softmax output layer and the target vector $\mathbf{y}$ is a one-hot vector with $1$ at $j$, $j\in{0,1,\dots 9}$ and $0$ for the rest indices.

The loss function $L(\mathbf{x};T)$ is, for $\mathbf{y}_k=1$:

\begin{align}
	L(\mathbf{x};T) &= \frac{\sum_{l=0}^9e^{\frac{\mathbf{x}_k}{T}}\mathbf{y}_l}{\sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}}} = \frac{e^{\frac{\mathbf{x}_k}{T}}}{\sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}}} \\
	\frac{\partial L(\mathbf{x};T)}{\partial \mathbf{x}_i} &= 
	\frac{1}{(\sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}})^2}
	(
	\frac{\partial e^{\frac{\mathbf{x}_k}{T}}}{\partial \mathbf{x}_i}
	\sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}} 
	+ e^{\frac{\mathbf{x}_k}{T}}
	\frac{\partial \sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}}}{\partial \mathbf{x}_i}
	) \nonumber \\
	&=
	\frac{1}{T(\sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}})^2} (
	\delta_{ik} e^{\frac{\mathbf{x}_k}{T}}
	\sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}}+
	e^{\frac{2\mathbf{x}_k}{T}} ) \nonumber 
\end{align}

Since the input vector $\mathbf{x}$ is always close to 0,

\begin{equation}
	\frac{\partial L(\mathbf{x};T)}{\partial \mathbf{x}_i}
	\approx \frac{\frac{\partial L(\mathbf{x};T=0)}{\partial \mathbf{x}_i}}{T} \text{ when } (\mid \mathbf{x}_i \mid \ll 1)
\end{equation}

Moreover, when $T$ approaches $(T \rightarrow \infty)$ infinity, we have 
\begin{align*}
	(\sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}})^{2T} -
	(\sum_{j=0}^9 e^{\mathbf{x}_j})^{2}
	&=((\sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}})^T+
	\sum_{j=0}^9 e^{\mathbf{x}_j})((\sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}})^T-\sum_{j=0}^9 e^{\mathbf{x}_j})\\
	&\ge((\sum_{j=0}^9 e^{\frac{\mathbf{x}_j}{T}})^T+
	\sum_{j=0}^9 e^{\mathbf{x}_j})\cdot 0 \ge 0 \hspace{10pt} (T > 1)\\
	e^{\frac{\mathbf{x}_j}{T}} &\le e^{\mathbf{x}_j}\\
	\text{Thus}\hspace{3cm} &\\
	\frac{\partial L(\mathbf{x};T)}{\partial \mathbf{x}_i} &\le 
	\frac{1}{T(\sum_{j=0}^9 e^{\mathbf{x}_j})^{\frac{2}{T}}} (
	\delta_{ik} e^{\mathbf{x}_k}
	\sum_{j=0}^9 e^{\mathbf{x}_j}+
	e^{\mathbf{x}_k} )\\
	&\le \frac{L(\mathbf{x},T=1)}{T(\sum_{j=0}^9 e^{\mathbf{x}_j})^{2(\frac{1}{T}-1)}}\\
	&\approx 
	\frac{L(\mathbf{x},T=1)}{T\cdot 10^{-2}} = 
	\frac{100L(\mathbf{x},T=1)}{T}
\end{align*}

The temperature as a term in denominator still determines the gradient when the temperature becomes much large.

We chose the temperature at $t=20$ as it is an appropriate setting at which both relatively high prediction accuracy and robustness are achieved by papernot's empirical study\cite{Papernot}. Most importantly, large temperature induces disastrous underflow. 

When the temperature is set to 20, the average gradient is already approximately $10^{-60}$. Only around $30\%$ percent of the sample can be used for numerical analysis. That is an important reason \textbf{algorithm 2.} is used in our work. 
Though the computational cost is relatively high but it is the only algorithm that could be analyzed numerically without recompiling the code to obtain higher precision or intricate modification of the computational graph.

\paragraph{Definition 3.} 
\textbf{Distillation training} is a procedure to, given a previously trained DNN model $M_1$ trained on original target $y_1$, feed the same input $x$ to a learning model $M_2$ to fit the \textbf{distilled target} $y_2=M_1(x)$.\cite{Ba}

Distillation training was initially designed to train a deep learning neural network on a resource restricted device such as phones and tablets. As the computational cost is too high to feed a gigantic database to a DNN with a large number of layers on such devices. 
Distillation training makes use of the knowledge leaned by another more powerful DNN and only use a small subset of the database with distilled targets to solve supervised learning problems.
As the distilled targets (similar as soft labels) extracted more information from the database compared to original targets (similar with hard labels), the distilled CNN could converge more rapidly by gradient descent or other optimization methods.

This could also raise problem while training as this might cause gradient explosion during the training and the training has to stop whenever this happens. This is usually handled by reducing the training rate or add a regularization term which decreases with time going.

\section{Results and Discussion}

\paragraph{Building CNN}The convolutional neural network chosen in our study is based on the architecture used by Papernot\cite{Papernot}.
 But, to avoid gradient explosion occurring when calculating the Jacobian of the loss function with high temperature during training process, the training rate is reduced to a half.

The testing accuracy for traditional training is \\
\begin{equation}
M_{std} \sim  99.57\%
\end{equation}

For distillation training using the same input as distilled targets produced by $M_{std}$, the testing accuracy is \\
\begin{equation}
M_{distill} \sim 99.07\%
\end{equation}

The comparison training model $M_{std-T}$ is a traditional training model except for the output layer using softmax function at temperature $20$. After training it with the original targets, the testing accuracy is a bit higher than $M_{distill}$.
\begin{equation}
M_{std-T} \sim 99.34 \%
\end{equation}

All the three models generalize considerably well, as proved by Papernot\cite{Papernot} using the theorems introduced by Shalev\cite{Shalev}.

\paragraph{Crafting Adversarial Samples}
We implemented \textbf{Algorithm 2} to craft adversarial samples due to its relatively high success rate of crafting a adversarial sample which could deceives a traditionally trained CNN , and most importantly to avoid underflow when calculating gradient on CNN with high temperature. Further study can attempt to implement \textbf{Algorithm 3} which requires some intricate modification for building the computational graph.

The success rate of adversarial attack against a standard or traditional CNN $(M_{std})$ is around $rate_{std}=37\%$ (35.69\% for $M_{distill}$ and 35.69\% for $M_{std-T}$ in two empirical tests) without excluding the samples causing $M_{distill}$ or $M_{std-T}$ to underflow. Similarly, $rate_{distill}=1.31\%$, $rate_{std-T}=0.81\%$. In summary:

\begin{table}[h!]

{\large \textbf{Adversarial Attacking Success Rate Comparison}}

\medskip
\begin{center}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} |c|c|} 
 \hline
		\hspace{1.5cm} Model \hspace{1.5cm} & \hspace{1cm}Relative Attack Success Rate \hspace{1cm} \\ 
\hline
$M_{std}$ & $rate_{std}/rate_{std}=1.00$ \\ 
\hline 
$M_{distill}$ & $rate_{std}/rate_{distill}=29.77$ \\ 
\hline 
$M_{std-T}$ & $rate_{std}/rate_{std-T}=44.30$ \\ 
\hline
\end{tabular*}
\end{center}
\caption{Adversarial attack success rate comparison}
\end{table}

Here is an example showing the effectiveness of defense against adversarial attacks for different CNN models.
We continue to perturb pixels one by one using \textbf{Algorithm 2} until the prediction produced by the CNN model differs from the target value.

\begin{figure}[h!]
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{original1.png}
		\caption{Original}
	\end{minipage} \hfill
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{std1.png}
		\caption{$M_{std}$}
	\end{minipage} 
\end{figure}
\newpage
\begin{figure}[h!]
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{distill1.png}
		\caption{$M_{distill}$}
	\end{minipage}
\end{figure}

\begin{figure}[h!]
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{O.png}
		\caption{Original}
	\end{minipage} \hfill
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{S.png}
		\caption{$M_{std}$}
	\end{minipage} 
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{T.png}
		\caption{$M_{std-T}$}
	\end{minipage} \hfill
\end{figure}

As illustrated by \textbf{Figure 3$\sim$8}, the success rate to deceive a traditionally trained CNN is $66\%$ when the maximum pixels to perturb is set to 100.
While both distillation training and high temperature traditional training improve the robustness of the CNN against adversarial perturbation and reduce the success rate to $2.22\%$ for distillation training and $1.49\%$ for high temperature traditional training from our empirical study.

Our empirical study suggests that the robustness against adversarial perturbation of the CNN trained by distillation at high temperature should result mainly from the high temperature rather than the usage of distillation technique.

\section{Conclusion}

To investigate the effectiveness of distillation as a defense to adversarial perturbation against convolutional neural network (CNN), we adopted the same CNN network used by \cite{Papernot} with modifications to avoid gradient explosion during training process and underflow during the testing process.

A traditional CNN $M_{std}$ was trained as a blank comparison model and model $M_{distill}$ was trained by distillation with the distilled targets produced by $M_{std}$.
As a comparison, another CNN $M_{std-T}$ of the same architecture without distillation was trained at the same temperature as the distillation model $(M_{distill})$.
We implemented the adversarial sample crafting \textbf{Algorithm 2} for relatively high success rate and low computational cost. Besides, this algorithm reduces the chance to underflow when evaluating saliency maps.

Our testing result shows that the two modified CNN models $M_{distill},M_{std-T}$ both generalize $(M_{distill} \sim 99.07\%$ , $M_{std-T} \sim 99.34\%)$ well compared to traditional training $(99.57\%)$ with slight decrease (less than 0.5\%). 
However, both model $M_{distill}$ and $M_{std-T}$ improve the robustness of the CNN by almost the same amount with $M_{std-T}$ slightly more robust than $M_{distill}$. This result suggests that the temperature of the output sofmax layer plays the most important role in reducing the success rate of adversarial attacks. Therefore, it leaves the effectiveness of distillation training in question.

Moreover, further study can investigate the robustness of CNN at much higher temperature $(T > 100)$ and test the adversarial sample crafting \textbf{Algorithm 3} with high precision and probably a certain amount of modifications to the building of computational graph.




\newpage
\appendix
\section{Sample Saliency Map for Traditionally Trained CNN}
mean gradient: -2.2915e-08\\
max gradient: 9.78318e-05\\
Perturbed: 12\\
Saliency map:\\
$[[$  1.96173855e-09   1.35970735e-09  -8.05801559e-09   3.28918093e-09 \\
   -5.48783348e-08  -7.17473085e-08  -4.38453718e-08  -5.30832978e-08 \\
   -2.52357182e-07  -1.88820479e-07  -1.97611584e-07  -1.68086672e-07 \\
   -7.32964693e-08  -1.42316367e-07  -1.02478126e-07  -1.31316881e-07 \\
   -1.58633611e-07  -1.31892534e-07  -1.97597899e-07  -2.12147782e-07 \\
   -1.20857607e-07  -1.28928747e-07  -4.98595369e-08  -4.03208240e-08 \\
   -7.82443266e-09  -1.32436873e-09   0.00000000e+00   0.00000000e+00 $]$ \\
    1.95072070e-09  -7.56662510e-10  -1.79181256e-08  -3.19850635e-09 \\
   -8.84076456e-08  -7.99879629e-08  -9.83269643e-08  -2.67942539e-07 \\
   -7.58413535e-07  -6.11258770e-07  -4.33601514e-07  -2.66673595e-07 \\
    7.28429370e-08   1.23588109e-08   7.01912199e-08  -3.30841772e-08 \\
   -1.88335321e-07  -1.84600680e-07  -2.85333499e-07  -2.19011724e-07 \\
    4.65144439e-08   1.59647158e-08   9.08492268e-08   6.10932176e-08 \\
	3.85807297e-08   2.76906196e-08   0.00000000e+00   0.00000000e+00 $]$ \\
   -4.20748947e-09  -2.18477370e-09  -3.84939938e-08  -4.47094806e-08 \\
   -5.14061682e-08   1.53504047e-07   9.17623311e-08   1.04581090e-08 \\
    8.77676030e-08   4.88882733e-07   5.50281470e-07   7.53525512e-07 \\
    9.84536655e-07   4.51604876e-07   2.71482520e-07  -1.81879841e-07 \\
    5.97312351e-08   3.29956265e-07   1.50822700e-07   3.56565351e-07 \\
    8.15367969e-07   5.19170669e-07   3.63578181e-07   1.58566408e-07 \\
    1.45516594e-07   7.73982194e-08  -1.83289606e-09  -7.77444054e-10$]$ \\
 $[$ -1.02055981e-08  -1.19342607e-08  -3.08573220e-08   3.72754307e-08 \\
    2.20618276e-07   6.08017331e-07   8.38857488e-07   6.92404910e-07 \\
    1.32143373e-06   1.64081018e-06   1.69421537e-06   2.58918908e-06 \\
    4.61905483e-06   3.51893982e-06   2.39151245e-06   5.98073598e-07 \\
    1.13812894e-06   1.60477293e-06   2.00945988e-06   2.09306722e-06 \\
    2.74859985e-06   2.79267442e-06   1.30998785e-06   5.38874133e-07 \\
    1.48021627e-07   6.51662404e-08  -4.27208824e-11  -3.48784424e-09$]$ \\
 $[$ -1.44070569e-08  -1.32951321e-08   9.07851643e-08   2.17700261e-07 \\
    2.62837204e-07   4.18497734e-07   6.46274998e-07   8.14089731e-07 \\
    3.08582366e-06   8.33536069e-06   1.30672488e-05   7.04140575e-06 \\
   -2.41726229e-06  -1.07430587e-05  -1.45426056e-05  -5.74765818e-06 \\
    1.78234131e-07   1.19687093e-06   1.99419901e-06   3.59732940e-06 \\
    6.97585074e-06   5.88091916e-06   2.81491452e-06  -7.44247046e-08 \\
    4.02008197e-07   1.79280406e-07   5.18654453e-10  -2.27380781e-10$]$ \\
 $[$  1.39619694e-08   7.94599799e-08   2.30915077e-07   1.45152370e-07 \\
   -1.62453290e-07   4.98938050e-07   1.09356279e-06   3.30207649e-06 \\
    5.32647027e-06   1.17240131e-06  -2.52363971e-05  -3.77056786e-05 \\
   -2.10110920e-05   2.28969529e-05   5.51288531e-06  -7.51445987e-06 \\
   $\dots \dots \dots ]]$

\newpage
\section{Sample Saliency Map for CNN with Temperature at 20}
mean gradient: -4.60666e-40\\
max gradient: 8.69376e-38\\
Perturbed: 29\\
$[$[ -6.82432352e-43  -3.63777081e-42  -1.27966576e-41  -3.99019738e-41\\
   -6.94091155e-41  -7.64982845e-41  -7.88314464e-41  -8.01360553e-41\\
   -6.43784540e-41  -5.24632132e-41  -5.45357336e-41  -6.26240284e-41\\
   -5.84705797e-41  -2.21531274e-41  -2.49290997e-42  -2.14272548e-41\\
   -2.29014208e-41   1.71659062e-42   2.30401494e-41   2.03118212e-41\\
   -1.20105291e-41  -4.23696604e-41  -4.21020124e-41  -2.27402715e-41\\
   -7.83325842e-42  -5.29690820e-43   1.37467379e-42   4.18988241e-43$]$\\
 $[$ -4.98441864e-42  -1.73690945e-41  -6.04898508e-41  -1.49134590e-40\\
   -2.37923664e-40  -2.42957128e-40  -2.51343899e-40  -2.64765536e-40\\
   -2.11076186e-40  -1.66848404e-40  -1.80167746e-40  -2.05524242e-40\\
   -1.82702695e-40  -7.99749060e-41  -3.26138205e-41  -1.30067122e-40\\
   -1.48216740e-40  -3.62319731e-41   6.64159420e-41   4.29021538e-41\\
   -8.63059724e-41  -1.88040241e-40  -1.84482344e-40  -1.01968285e-40\\
   -2.87854731e-41  -1.11403228e-42   2.18322301e-42   5.77334967e-43$]$\\
 $[$ -1.30544965e-41  -4.48289392e-41  -8.68931165e-41  -1.67659756e-40\\
   -2.21584524e-40  -1.57075749e-40  -1.35002495e-40  -1.62844895e-40\\
   -1.39821561e-40  -1.16907528e-40  -1.56899185e-40  -2.04048675e-40\\
   -1.50052441e-40  -1.27896511e-41  -1.46015300e-42  -1.71430650e-40\\
   -2.10676816e-40  -1.40255963e-41   1.21356651e-40   3.94871894e-41\\
   -1.17525501e-40  -1.99155340e-40  -1.48655346e-40  -4.39461211e-41\\
    2.31788779e-41   1.73859100e-41   1.89315423e-42  -4.51218106e-43$]$\\
 $[$ -3.67238289e-41  -1.06619195e-40  -7.38974745e-41   3.68415379e-41 \\
 1.91769096e-40   3.67316761e-40   4.42441773e-40   3.60479826e-40\\
    1.57591427e-40   1.96044458e-40   6.49050620e-40   5.17073528e-40\\
   -6.65555113e-40  -1.68051419e-39  -2.50207586e-39  -3.95834726e-39\\
   -5.42554599e-39  -6.21457092e-39  -5.94773987e-39  -4.21364563e-39\\
   -2.21070948e-39  -5.45098096e-40   7.77510453e-41   2.04819389e-40\\
    1.12713442e-40   1.72317672e-41  -1.67497205e-41  -4.30058499e-42$]$\\
 $[$ -4.75628725e-41  -1.28367347e-40  -6.36539827e-41   1.42966075e-40\\
    3.62727509e-40   5.08650323e-40   6.05152143e-40   4.18591673e-40\\
   -1.20262237e-40   3.05464848e-40   2.18003926e-39   1.72078330e-39\\
   -2.77900187e-39  -8.29900518e-39  -1.22252893e-38  -1.73823493e-38\\
   -2.55598409e-38  -2.91966420e-38  -2.94271276e-38  -2.03610180e-38\\
 \dots \dots \dots $]]$\\
 \newpage
\bibliography{Papernot,Papernot2,MNIST,Lasagne,Theano,Ba,Shalev,schmid} 
\bibliographystyle{IEEEtran}
\end{document}
